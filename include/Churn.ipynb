{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1d0f1448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y0/404pxmfn357ftb9snt8c2x4r0000gn/T/ipykernel_98076/3333807376.py:27: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  date = pd.to_datetime(df[\"CreadoEl\"], dayfirst=True, errors=\"coerce\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Definir ruta del dataset\n",
    "CSV_PATH = \"ventas_2025-10-02.csv\"  \n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Par√°metros din√°micos para el churn\n",
    "ALPHA = 3.0\n",
    "MIN_DAYS = 15\n",
    "MAX_DAYS = 180\n",
    "\n",
    "# Cargar dataset\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Combinar fecha y hora en una sola columna de tipo datetime\n",
    "date = pd.to_datetime(df[\"CreadoEl\"], dayfirst=True, errors=\"coerce\")\n",
    "hora = df[\"Hora\"].astype(str).str.strip()\n",
    "hora = np.where(hora.str.match(r\"^\\d{1,2}:\\d{2}$\"), hora + \":00\", hora)\n",
    "time_delta = pd.to_timedelta(hora, errors=\"coerce\")\n",
    "df[\"dt\"] = date + time_delta.fillna(pd.Timedelta(0))\n",
    "\n",
    "# Limpiar valores de importe\n",
    "def to_number_vec(s):\n",
    "    s = s.astype(str).str.replace(r\"[.\\s$ARSa-zA-Z]\", \"\", regex=True)\n",
    "    s = s.str.replace(\",\", \".\", regex=False)\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "df[\"ValorNeto_num\"] = to_number_vec(df[\"ValorNeto\"])\n",
    "\n",
    "# Definir columna de cliente\n",
    "key_col = \"Cliente\" if \"Cliente\" in df.columns else \"CUIT\"\n",
    "\n",
    "# Ordenar por tiempo\n",
    "df = df.sort_values([key_col, \"dt\"]).reset_index(drop=True)\n",
    "cutoff_dt = df[\"dt\"].max() # La fecha de corte es la fecha de la ultima venta en el dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ea27a4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diferencia en d√≠as entre compras por cliente y se hace un promedio\n",
    "df[\"diff_days\"] = df.groupby(key_col)[\"dt\"].diff().dt.days\n",
    "mean_gap = df.groupby(key_col)[\"diff_days\"].mean().rename(\"mean_gap\")\n",
    "global_median_gap = df[\"diff_days\"].dropna().median()\n",
    "mean_gap = mean_gap.fillna(global_median_gap)\n",
    "\n",
    "# Umbral din√°mico: multiplica su frecuencia normal por ALPHA, limitado por MIN_DAYS y MAX_DAYS\n",
    "dyn_threshold = (ALPHA * mean_gap).clip(lower=MIN_DAYS, upper=MAX_DAYS).rename(\"dyn_threshold\")\n",
    "\n",
    "# D√≠as desde la √∫ltima compra: buscamos ultima fecha de compra por cliente y calculamos diferencia con fecha de corte (esto ser√≠a la recencia)\n",
    "last_purchase = df.groupby(key_col)[\"dt\"].max().rename(\"last_purchase_dt\")\n",
    "days_since_last = (cutoff_dt - last_purchase).dt.days.rename(\"days_since_last\")\n",
    "\n",
    "# Etiqueta churn: 1 si super√≥ el umbral din√°mico. Variable objetivo.\n",
    "label = (days_since_last > dyn_threshold).astype(int).rename(\"churn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5cedc208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cliente_id</th>\n",
       "      <th>n_purchases</th>\n",
       "      <th>ValorNeto_num_sum</th>\n",
       "      <th>ValorNeto_num_mean</th>\n",
       "      <th>ValorNeto_num_std</th>\n",
       "      <th>ValorNeto_num_median</th>\n",
       "      <th>Vendedor_nunique</th>\n",
       "      <th>Oficina_nunique</th>\n",
       "      <th>Localidad_nunique</th>\n",
       "      <th>Localidad_Moda</th>\n",
       "      <th>...</th>\n",
       "      <th>Nombre1_Moda</th>\n",
       "      <th>mean_gap</th>\n",
       "      <th>dyn_threshold</th>\n",
       "      <th>days_since_last</th>\n",
       "      <th>churn</th>\n",
       "      <th>tenure_days</th>\n",
       "      <th>purchases_per_90d</th>\n",
       "      <th>recency_days</th>\n",
       "      <th>monetary_mean</th>\n",
       "      <th>monetary_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1817</td>\n",
       "      <td>1.403819e+09</td>\n",
       "      <td>772602.559774</td>\n",
       "      <td>729299.429540</td>\n",
       "      <td>690083.880</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PALMIRA</td>\n",
       "      <td>...</td>\n",
       "      <td>GONZALEZ MARIA LAURA</td>\n",
       "      <td>0.350220</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>636</td>\n",
       "      <td>257.122642</td>\n",
       "      <td>0</td>\n",
       "      <td>772602.559774</td>\n",
       "      <td>1.403819e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>2122</td>\n",
       "      <td>1.717844e+09</td>\n",
       "      <td>809539.847743</td>\n",
       "      <td>767350.064752</td>\n",
       "      <td>654888.945</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>SAN MARTIN</td>\n",
       "      <td>...</td>\n",
       "      <td>VALESTRA, MARIO ALBERTO</td>\n",
       "      <td>0.300330</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>637</td>\n",
       "      <td>299.811617</td>\n",
       "      <td>0</td>\n",
       "      <td>809539.847743</td>\n",
       "      <td>1.717844e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>1114</td>\n",
       "      <td>4.824796e+08</td>\n",
       "      <td>433105.589632</td>\n",
       "      <td>350738.538884</td>\n",
       "      <td>387101.125</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>GODOY CRUZ</td>\n",
       "      <td>...</td>\n",
       "      <td>GO√ëI FLORES JOSE IGNACIO</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>630</td>\n",
       "      <td>159.142857</td>\n",
       "      <td>0</td>\n",
       "      <td>433105.589632</td>\n",
       "      <td>4.824796e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>1640</td>\n",
       "      <td>1.132321e+09</td>\n",
       "      <td>690439.607409</td>\n",
       "      <td>658398.985418</td>\n",
       "      <td>604659.995</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>RIVADAVIA</td>\n",
       "      <td>...</td>\n",
       "      <td>RINI, LUIS ARTURO</td>\n",
       "      <td>0.388652</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>637</td>\n",
       "      <td>231.711146</td>\n",
       "      <td>0</td>\n",
       "      <td>690439.607409</td>\n",
       "      <td>1.132321e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>2028</td>\n",
       "      <td>3.681706e+08</td>\n",
       "      <td>181543.700192</td>\n",
       "      <td>244441.608950</td>\n",
       "      <td>82389.805</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PALMIRA</td>\n",
       "      <td>...</td>\n",
       "      <td>MA√ëUECO, ADRIANA LUCIA</td>\n",
       "      <td>0.314258</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>637</td>\n",
       "      <td>286.530612</td>\n",
       "      <td>0</td>\n",
       "      <td>181543.700192</td>\n",
       "      <td>3.681706e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cliente_id  n_purchases  ValorNeto_num_sum  ValorNeto_num_mean  \\\n",
       "0           5         1817       1.403819e+09       772602.559774   \n",
       "1           8         2122       1.717844e+09       809539.847743   \n",
       "2           9         1114       4.824796e+08       433105.589632   \n",
       "3          20         1640       1.132321e+09       690439.607409   \n",
       "4          23         2028       3.681706e+08       181543.700192   \n",
       "\n",
       "   ValorNeto_num_std  ValorNeto_num_median  Vendedor_nunique  Oficina_nunique  \\\n",
       "0      729299.429540            690083.880                 1                1   \n",
       "1      767350.064752            654888.945                 1                1   \n",
       "2      350738.538884            387101.125                 1                1   \n",
       "3      658398.985418            604659.995                 1                1   \n",
       "4      244441.608950             82389.805                 1                1   \n",
       "\n",
       "   Localidad_nunique Localidad_Moda  ...              Nombre1_Moda  mean_gap  \\\n",
       "0                  1        PALMIRA  ...      GONZALEZ MARIA LAURA  0.350220   \n",
       "1                  1     SAN MARTIN  ...   VALESTRA, MARIO ALBERTO  0.300330   \n",
       "2                  1     GODOY CRUZ  ...  GO√ëI FLORES JOSE IGNACIO  0.566038   \n",
       "3                  1      RIVADAVIA  ...         RINI, LUIS ARTURO  0.388652   \n",
       "4                  1        PALMIRA  ...    MA√ëUECO, ADRIANA LUCIA  0.314258   \n",
       "\n",
       "   dyn_threshold  days_since_last  churn  tenure_days  purchases_per_90d  \\\n",
       "0           15.0                0      0          636         257.122642   \n",
       "1           15.0                0      0          637         299.811617   \n",
       "2           15.0                0      0          630         159.142857   \n",
       "3           15.0                0      0          637         231.711146   \n",
       "4           15.0                0      0          637         286.530612   \n",
       "\n",
       "   recency_days  monetary_mean  monetary_sum  \n",
       "0             0  772602.559774  1.403819e+09  \n",
       "1             0  809539.847743  1.717844e+09  \n",
       "2             0  433105.589632  4.824796e+08  \n",
       "3             0  690439.607409  1.132321e+09  \n",
       "4             0  181543.700192  3.681706e+08  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHURN.IPYNB - CELDA 37 (C√ìDIGO MODIFICADO Y CORREGIDO)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Asumiendo que key_col, cutoff_dt, df est√°n definidos en celdas previas\n",
    "\n",
    "# --- 1. Funci√≥n para calcular la Moda ---\n",
    "def mode_agg(x):\n",
    "    \"\"\"Calcula el valor m√°s frecuente (moda), ignorando NaNs.\"\"\"\n",
    "    return x.mode()[0] if not x.mode().empty else np.nan\n",
    "\n",
    "# --- 2. Definici√≥n del diccionario de agregaci√≥n (incluye Moda) ---\n",
    "agg_dict = {\n",
    "    \"Documento\": \"count\",\n",
    "    \"ValorNeto_num\": [\"sum\", \"mean\", \"std\", \"median\"],\n",
    "    \"Vendedor\": pd.Series.nunique,\n",
    "    \"Oficina\": pd.Series.nunique,\n",
    "    \"Localidad\": [pd.Series.nunique, mode_agg], # <-- A√ëADIDO: Moda de Localidad\n",
    "    \"Nombre1\": [pd.Series.nunique, mode_agg],   # <-- A√ëADIDO: Moda de Nombre1\n",
    "    \"dt\": \"min\" # Se agrega la fecha m√≠nima para antig√ºedad\n",
    "}\n",
    "\n",
    "# --- 3. Agregaci√≥n y renombramiento de columnas ---\n",
    "features = df.groupby(key_col).agg(agg_dict)\n",
    "\n",
    "# Aplanar el multi-√≠ndice de columnas\n",
    "features.columns = [\n",
    "    \"_\".join(c).replace(\"Documento_count\", \"n_purchases\") \n",
    "    if isinstance(c, tuple) else c \n",
    "    for c in features.columns\n",
    "]\n",
    "\n",
    "# Renombrar las nuevas columnas de moda para claridad\n",
    "features = features.rename(columns={\n",
    "    'Localidad_mode_agg': 'Localidad_Moda',\n",
    "    'Nombre1_mode_agg': 'Nombre1_Moda',\n",
    "    'dt_min': 'first_purchase_dt' # Renombrar la primera fecha de compra\n",
    "})\n",
    "# Las columnas nunique tambi√©n se renombran si se aplicaron m√∫ltiples agregaciones\n",
    "# Ej: 'Localidad_nunique'\n",
    "\n",
    "# --- 4. Concatenaci√≥n con variables de tiempo y Churn (CORRECCI√ìN CLAVE) ---\n",
    "# Usamos las Series 'mean_gap', 'dyn_threshold', 'days_since_last', y 'label' directamente.\n",
    "# El error 'df_model is not defined' se corrige usando las variables del notebook.\n",
    "cust_df = pd.concat([\n",
    "    features, \n",
    "    mean_gap, \n",
    "    dyn_threshold, \n",
    "    last_purchase, \n",
    "    days_since_last, \n",
    "    label\n",
    "], axis=1)\n",
    "\n",
    "# Se elimina la l√≠nea de join de first_purchase porque ya est√° en 'features'\n",
    "\n",
    "# --- 5. Agregar features adicionales (Antig√ºedad y Frecuencia Normalizada) ---\n",
    "cust_df[\"tenure_days\"] = (cutoff_dt - cust_df[\"first_purchase_dt\"]).dt.days\n",
    "cust_df[\"purchases_per_90d\"] = cust_df[\"n_purchases\"] / (cust_df[\"tenure_days\"] / 90.0).replace(0, np.nan)\n",
    "cust_df[\"recency_days\"] = cust_df[\"days_since_last\"]\n",
    "cust_df[\"monetary_mean\"] = cust_df.get(\"ValorNeto_num_mean\", np.nan)\n",
    "cust_df[\"monetary_sum\"] = cust_df.get(\"ValorNeto_num_sum\", np.nan)\n",
    "\n",
    "# Limpieza final de columnas temporales\n",
    "cust_df = cust_df.drop(columns=['last_purchase_dt', 'first_purchase_dt'], errors='ignore') # Se eliminan las columnas de fecha datetime\n",
    "\n",
    "cust_df = cust_df.reset_index().rename(columns={key_col: \"cliente_id\"})\n",
    "cust_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b94c5f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rellenando NaN con la mediana antes de guardar...\n",
      "\n",
      "‚úÖ DataFrame 'cust_df_final_for_streamlit.csv' guardado exitosamente.\n",
      "N√∫mero de filas: 1145, N√∫mero de columnas: 21\n",
      "Este archivo debe ser utilizado en la aplicaci√≥n Streamlit.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Definici√≥n del nombre del archivo ---\n",
    "DATA_FILENAME = 'cust_df_final_for_streamlit.csv'\n",
    "\n",
    "# --- 2. Preparaci√≥n del DataFrame (Asegurar que solo tenga las columnas correctas) ---\n",
    "\n",
    "# NOTA: Aseg√∫rate de que 'cust_df' est√° definido en tu entorno.\n",
    "# Si 'cust_df' incluye columnas intermedias (`days_since_last`, `dyn_threshold`),\n",
    "# es mejor excluirlas antes de guardar, aunque Streamlit solo usar√° las columnas de features.\n",
    "\n",
    "# Si quieres ser muy estricto con las columnas:\n",
    "# 1. Obt√©n la lista de features usadas (X_cols) y a√±ade 'cliente_id'\n",
    "#    Asumimos que X_cols y el target_col ('churn') est√°n definidos en celdas previas.\n",
    "#\n",
    "#    cols_to_keep = ['cliente_id'] + X_cols + ['churn'] \n",
    "#    df_to_save = cust_df[cols_to_keep]\n",
    "\n",
    "# Si asumes que cust_df est√° limpio, simplemente lo guardamos:\n",
    "df_to_save = cust_df.copy()\n",
    "\n",
    "# --- 3. Limpieza y Conversi√≥n ---\n",
    "\n",
    "# Convertir la columna 'cliente_id' a string para asegurar consistencia en Streamlit.\n",
    "if 'cliente_id' in df_to_save.columns:\n",
    "    df_to_save['cliente_id'] = df_to_save['cliente_id'].astype(str)\n",
    "\n",
    "# Rellenar cualquier NaN que pudiera haber quedado. Streamlit lo necesita limpio.\n",
    "# Usamos el mismo m√©todo que en el entrenamiento (mediana).\n",
    "# Esto es crucial para asegurar que el archivo guardado es usable.\n",
    "print(\"Rellenando NaN con la mediana antes de guardar...\")\n",
    "df_to_save = df_to_save.fillna(df_to_save.median(numeric_only=True))\n",
    "\n",
    "\n",
    "# --- 4. Guardar el DataFrame ---\n",
    "try:\n",
    "    df_to_save.to_csv(DATA_FILENAME, index=False)\n",
    "    print(f\"\\n‚úÖ DataFrame '{DATA_FILENAME}' guardado exitosamente.\")\n",
    "    print(f\"N√∫mero de filas: {df_to_save.shape[0]}, N√∫mero de columnas: {df_to_save.shape[1]}\")\n",
    "    print(\"Este archivo debe ser utilizado en la aplicaci√≥n Streamlit.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Error al guardar el DataFrame: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f524ec30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROC_AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HistGradientBoosting</th>\n",
       "      <td>0.998617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.998490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.996399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ROC_AUC\n",
       "HistGradientBoosting  0.998617\n",
       "Logistic Regression   0.998490\n",
       "Random Forest         0.996399"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos la columna objetivo\n",
    "target_col = \"churn\"\n",
    "\n",
    "# Divisi√≥n entrenamiento y test \n",
    "X = cust_df.select_dtypes(include=[np.number]).drop(columns=[\"churn\"], errors=\"ignore\")\n",
    "y = cust_df[target_col].astype(int)\n",
    "\n",
    "X = X.fillna(X.median(numeric_only=True))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Preprocesamiento (escalado) y definici√≥n de modelos\n",
    "preprocess = ColumnTransformer([(\"num\", StandardScaler(), X.columns)], remainder=\"drop\")\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE),\n",
    "    \"HistGradientBoosting\": HistGradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "}\n",
    "\n",
    "# Validaci√≥n cruzada estratificada y evaluaci√≥n de modelos\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Pipeline de entrenamiento y evaluaci√≥n por modelo \n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline([(\"prep\", preprocess), (\"clf\", model)])\n",
    "    scores = cross_validate(pipe, X_train, y_train, scoring=\"roc_auc\", cv=cv, n_jobs=-1)\n",
    "    results[name] = scores[\"test_score\"].mean()\n",
    "\n",
    "pd.DataFrame.from_dict(results, orient=\"index\", columns=[\"ROC_AUC\"]).sort_values(by=\"ROC_AUC\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a8c40a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/facundo/Documents/proyecto/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>roc_auc_mean</th>\n",
       "      <th>f1_mean</th>\n",
       "      <th>precision_mean</th>\n",
       "      <th>recall_mean</th>\n",
       "      <th>accuracy_mean</th>\n",
       "      <th>roc_auc_train_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.900363</td>\n",
       "      <td>0.523939</td>\n",
       "      <td>0.699937</td>\n",
       "      <td>0.436190</td>\n",
       "      <td>0.913744</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HistGradientBoosting</td>\n",
       "      <td>0.896297</td>\n",
       "      <td>0.491138</td>\n",
       "      <td>0.587357</td>\n",
       "      <td>0.436190</td>\n",
       "      <td>0.901746</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.885773</td>\n",
       "      <td>0.101507</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.058571</td>\n",
       "      <td>0.887568</td>\n",
       "      <td>0.887267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  roc_auc_mean   f1_mean  precision_mean  recall_mean  \\\n",
       "1         Random Forest      0.900363  0.523939        0.699937     0.436190   \n",
       "2  HistGradientBoosting      0.896297  0.491138        0.587357     0.436190   \n",
       "0   Logistic Regression      0.885773  0.101507        0.383333     0.058571   \n",
       "\n",
       "   accuracy_mean  roc_auc_train_mean  \n",
       "1       0.913744            1.000000  \n",
       "2       0.901746            1.000000  \n",
       "0       0.887568            0.887267  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rehacemos el entrenamiento y la comparaci√≥n de modelos excluyendo variables con leakage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Construir X,y SI NO ESTUVIESEN ya definidos (caso autosuficiente):\n",
    "if 'cust_df' in globals():\n",
    "    target_col = 'churn'\n",
    "    candidate_num = [c for c in cust_df.columns \n",
    "                     if c not in ['cliente_id','churn','last_purchase_dt','first_purchase_dt'] \n",
    "                     and np.issubdtype(cust_df[c].dtype, np.number)]\n",
    "    X_all = cust_df[candidate_num].copy()\n",
    "    y_all = cust_df[target_col].astype(int).copy()\n",
    "else:\n",
    "    X_all = X.copy()\n",
    "    y_all = y.copy()\n",
    "\n",
    "# Quitar variables con leakage (derivadas del label)\n",
    "leaky_cols = [c for c in ['days_since_last','dyn_threshold','mean_gap','recency_days'] if c in X_all.columns]\n",
    "X_safe = X_all.drop(columns=leaky_cols)\n",
    "\n",
    "# Imputaci√≥n simple y split\n",
    "X_safe = X_safe.replace([np.inf, -np.inf], np.nan)\n",
    "X_safe = X_safe.fillna(X_safe.median(numeric_only=True))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_safe, y_all, test_size=0.2, stratify=y_all, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Preprocesamiento + modelos\n",
    "preprocess = ColumnTransformer([(\"num\", StandardScaler(), X_safe.columns)], remainder=\"drop\")\n",
    "\n",
    "models_safe = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE),\n",
    "    \"HistGradientBoosting\": HistGradientBoostingClassifier(random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Definici√≥n de pipeline y evaluaci√≥n\n",
    "rows = []\n",
    "for name, model in models_safe.items():\n",
    "    pipe = Pipeline([(\"prep\", preprocess), (\"clf\", model)])\n",
    "    scores = cross_validate(\n",
    "        pipe, X_train, y_train,\n",
    "        scoring=[\"roc_auc\", \"f1\", \"precision\", \"recall\", \"accuracy\"],\n",
    "        cv=cv, n_jobs=-1, return_train_score=True\n",
    "    )\n",
    "    rows.append({\n",
    "        \"model\": name,\n",
    "        \"roc_auc_mean\": scores[\"test_roc_auc\"].mean(),\n",
    "        \"f1_mean\": scores[\"test_f1\"].mean(),\n",
    "        \"precision_mean\": scores[\"test_precision\"].mean(),\n",
    "        \"recall_mean\": scores[\"test_recall\"].mean(),\n",
    "        \"accuracy_mean\": scores[\"test_accuracy\"].mean(),\n",
    "        \"roc_auc_train_mean\": scores[\"train_roc_auc\"].mean()\n",
    "    })\n",
    "\n",
    "comp_safe = pd.DataFrame(rows).sort_values(\"roc_auc_mean\", ascending=False)\n",
    "comp_safe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dc2f6ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo seleccionado para optimizaci√≥n: Random Forest\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "**Ajuste de Umbral**: Buscando el equilibrio con un umbral de 0.25\n",
      "== Test metrics (Umbral ajustado a 0.25) ==\n",
      "ROC-AUC : 0.9178\n",
      "Accuracy: 0.8821\n",
      "Precision: 0.4839  Recall: 0.5769  F1: 0.5263\n",
      "Brier score (MSE probas): 0.0612  |  RMSE probas: 0.2473\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.944     0.921     0.933       203\n",
      "           1      0.484     0.577     0.526        26\n",
      "\n",
      "    accuracy                          0.882       229\n",
      "   macro avg      0.714     0.749     0.729       229\n",
      "weighted avg      0.892     0.882     0.887       229\n",
      "\n",
      "Confusion matrix (FN=15 / FP=10. Deber√≠a mejorar el equilibrio):\n",
      " [[187  16]\n",
      " [ 11  15]]\n",
      "Keys disponibles en cross_validate: ['fit_time', 'score_time', 'test_score', 'train_score']\n",
      "\n",
      "== Diagn√≥stico CV (ROC-AUC) ==\n",
      "Train AUC mean : 0.9956\n",
      "Valid AUC mean : 0.9067\n",
      "\n",
      " Posible OVERFITTING (train >> valid)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 0. DEFINICIONES NECESARIAS (Asumimos que las librer√≠as est√°n cargadas)\n",
    "# ==============================================================================\n",
    "# --- Importaciones (Se mantienen) ---\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, RandomizedSearchCV, cross_validate\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, roc_auc_score,\n",
    "    brier_score_loss, classification_report, confusion_matrix, f1_score\n",
    ")\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# 1. Definir Constantes y Objetos\n",
    "RANDOM_STATE = 42\n",
    "# Se asume que X_train est√° definido\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), X_train.columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE) \n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# üåü MODIFICACI√ìN CLAVE 1: FORZAR LA SELECCI√ìN DE RANDOM FOREST\n",
    "# (Basado en la tabla adjunta donde tiene el mejor 'roc_auc_mean' y 'roc_auc_train_mean')\n",
    "# ----------------------------------------------------------------------------\n",
    "best_name = \"Random Forest\"\n",
    "print(f\"Modelo seleccionado para optimizaci√≥n: {best_name}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# BLOQUE DE OPTIMIZACI√ìN DE MODELO\n",
    "# ==============================================================================\n",
    "\n",
    "if best_name == \"Random Forest\":\n",
    "    # üåü MODIFICACI√ìN CLAVE 2: SE USA EL CLASIFICADOR Y PARAMETROS DE RANDOM FOREST\n",
    "    base = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "    \n",
    "    # Par√°metros para Random Forest. Limitando max_depth para ayudar a mitigar overfitting\n",
    "    param_dist = {\n",
    "        \"clf__n_estimators\": randint(300, 1000), # Rango ajustado\n",
    "        \"clf__max_depth\": randint(3, 16),        # Limitando profundidad para regularizaci√≥n\n",
    "        \"clf__min_samples_split\": randint(2, 10),\n",
    "        \"clf__min_samples_leaf\": randint(1, 5),  # Valores bajos para mejor rendimiento\n",
    "        \"clf__max_features\": [\"sqrt\", \"log2\", None],\n",
    "    }\n",
    "    n_iter_search = 50\n",
    "    \n",
    "elif best_name == \"HistGradientBoosting\":\n",
    "    base = HistGradientBoostingClassifier(random_state=RANDOM_STATE)\n",
    "    param_dist = {\n",
    "        \"clf__max_depth\": randint(3, 12), \n",
    "        \"clf__learning_rate\": uniform(0.01, 0.1), \n",
    "        \"clf__max_leaf_nodes\": randint(15, 63),\n",
    "        \"clf__l2_regularization\": uniform(0.0, 2.0),\n",
    "    }\n",
    "    n_iter_search = 50 \n",
    "else:\n",
    "    base = LogisticRegression(max_iter=500)\n",
    "    param_dist = {\"clf__C\": uniform(0.01, 10.0)}\n",
    "    n_iter_search = 50\n",
    "\n",
    "tune_pipe = Pipeline([(\"prep\", preprocess), (\"clf\", base)])\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=tune_pipe, \n",
    "    param_distributions=param_dist,\n",
    "    n_iter=n_iter_search, \n",
    "    # üåü Mantenemos 'f1' como m√©trica de scoring para optimizar el equilibrio de clases\n",
    "    scoring=\"f1\", \n",
    "    cv=cv, \n",
    "    n_jobs=-1, \n",
    "    random_state=RANDOM_STATE, \n",
    "    verbose=1\n",
    ")\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "best_pipe = search.best_estimator_\n",
    "\n",
    "# Evaluaci√≥n exhaustiva en TEST\n",
    "\n",
    "# Probabilidades para AUC, Brier y ajuste de umbral\n",
    "try:\n",
    "    y_proba = best_pipe.predict_proba(X_test)[:, 1]\n",
    "except Exception:\n",
    "    from sklearn.utils.extmath import softmax\n",
    "    dec = best_pipe.decision_function(X_test)\n",
    "    y_proba = (dec - dec.min()) / (dec.max() - dec.min() + 1e-9)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# üåü AJUSTE DEL UMBRAL PARA ENCONTRAR EL EQUILIBRIO (0.25)\n",
    "# -------------------------------------------------------------\n",
    "# Mantenemos el umbral bajo para favorecer la reducci√≥n de Falsos Negativos.\n",
    "threshold = 0.25 \n",
    "print(f\"**Ajuste de Umbral**: Buscando el equilibrio con un umbral de {threshold}\")\n",
    "\n",
    "y_pred = (y_proba >= threshold).astype(int)\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "\n",
    "# M√©tricas basadas en el nuevo umbral (Resto del c√≥digo de evaluaci√≥n)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\", zero_division=0)\n",
    "auc = roc_auc_score(y_test, y_proba) \n",
    "brier = brier_score_loss(y_test, y_proba)       \n",
    "rmse_prob = np.sqrt(brier)                       \n",
    "\n",
    "print(\"== Test metrics (Umbral ajustado a %.2f) ==\" % threshold)\n",
    "print(f\"ROC-AUC : {auc:.4f}\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}  Recall: {rec:.4f}  F1: {f1:.4f}\")\n",
    "print(f\"Brier score (MSE probas): {brier:.4f}  |  RMSE probas: {rmse_prob:.4f}\")\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "print(\"Confusion matrix (FN=15 / FP=10. Deber√≠a mejorar el equilibrio):\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Diagn√≥stico de Over/Under-fitting con CV en TRAIN\n",
    "cv_diag = cross_validate(\n",
    "    best_pipe,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "keys = list(cv_diag.keys())\n",
    "print(\"Keys disponibles en cross_validate:\", keys)\n",
    "\n",
    "train_key = next((k for k in keys if \"train\" in k), None)\n",
    "test_key = next((k for k in keys if \"test\" in k), None)\n",
    "\n",
    "train_auc = np.mean(cv_diag[train_key])\n",
    "valid_auc = np.mean(cv_diag[test_key])\n",
    "\n",
    "print(\"\\n== Diagn√≥stico CV (ROC-AUC) ==\")\n",
    "print(f\"Train AUC mean : {train_auc:.4f}\")\n",
    "print(f\"Valid AUC mean : {valid_auc:.4f}\")\n",
    "\n",
    "gap = train_auc - valid_auc\n",
    "\n",
    "if gap > 0.03:\n",
    "    print(\"\\n Posible OVERFITTING (train >> valid)\")\n",
    "elif valid_auc < 0.65:\n",
    "    print(\"\\n Posible UNDERFITTING (validaci√≥n baja).\")\n",
    "else:\n",
    "    print(\"\\nAjuste razonable (sin se√±ales fuertes de over/under-fitting).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a86fd977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo y lista de features guardados exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline # Necesitas definir best_pipe aqu√≠\n",
    "\n",
    "# Aseg√∫rate de que 'best_pipe' es tu modelo final entrenado\n",
    "# y 'X_train.columns' contiene los nombres de las features usadas.\n",
    "\n",
    "# 1. Guardar el modelo Pipeline (que incluye preprocesamiento)\n",
    "joblib.dump(best_pipe, 'modelo_churn_final.joblib')\n",
    "\n",
    "# 2. Guardar la lista de columnas (features) usadas por el modelo. Esto es CR√çTICO.\n",
    "feature_names = X_train.columns.tolist()\n",
    "joblib.dump(feature_names, 'features_list.joblib')\n",
    "\n",
    "print(\"Modelo y lista de features guardados exitosamente.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
